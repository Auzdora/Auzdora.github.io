<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Query Processing</title>
      <link href="/2023/07/01/QueryProcessing/"/>
      <url>/2023/07/01/QueryProcessing/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Query Processing指的是从数据库抽取数据的一系列活动。是用户和物理存储底层交互的中间层，其中包含三个阶段：</p><ul><li><strong>解析和翻译（Parsing &amp; translation）</strong>：解析用户的意图（通过构造Abstract Syntax Tree），并与DBMS内部的数据结构相对应（通过binder）然后进行翻译</li><li><strong>优化（Optimization）</strong>：对Query流程进行优化，找到开销最小的执行</li><li><strong>评估（Evaluation）</strong>：对一个Query会有不同的优化结果，定量评估其开销</li></ul></blockquote><p>我们可以先对这三个阶段在high-level的角度理解，之后在深入到具体的算法细节。</p><h2 id="High-level-Overview"><a href="#High-level-Overview" class="headerlink" title="High-level Overview"></a>High-level Overview</h2><p>一个Query在执行之前，需要先转化为系统理解的方式。SQL语句是声明式语言，意思就是仅仅告诉DBMS我需要什么数据。而DBMS把如何获取数据、那种方式最优等等这一系列问题隐藏起来。SQL是为了方便User使用的一个接口，为了让DBMS理解它，就需要和编译器一样对句法进行解析。</p><p>在这其中。解析器会做一些检查，比如用户的句法是否被DBMS定义、是否正确、是否能在DBMS中找到对应的数据结构等等。构建AST之后，它便会将其翻译成关系代数表达式，方便后续的优化。</p><p>用户的一个Query可以被翻译为不同的关系代数表达式。而每一个表达式中的操作算子又可以采用很多种不同的算法实现。如果想评估一个query，我们不仅仅需要关系代数表达式，还需要标注一些评估算子开销的必要信息。一个标有相关信息的关系代数算子成为evaluation primitive；一些列被标注的算子构成的query关系代数表达式被称为一个query plan。</p><p>优化的目的就是从很多种query plan中找到开销最小的plan，并把plan送给execution engine执行。</p><p>下图是书“Database System Concepts”中的图，可以更好的理解Query Processing在数据库中的流程：</p><p><img src="query1.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> Database Management System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cast_in_cpp</title>
      <link href="/2023/06/23/cast-in-cpp/"/>
      <url>/2023/06/23/cast-in-cpp/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>在用C++写一些Lab和项目的时候，涉及到cast在C++里的应用。发现自己并不熟悉这些东西，于是也仅仅在阅读一些博客后草草了事，没有很深的记忆。但是过一段时间之后，又全部都忘记了，所以想写一个博客来记录一下加深自己的理解。</p></blockquote><p>C++是一种强类型语言，当需要把一个变量按照使用需求转化为另一个变量的时候，就需要用到类型转换。C++提供了四种类型转换的方法：</p><ul><li>static_cast</li><li>dynamic_cast</li><li>reinterpret_cast</li><li>const_cast</li></ul><p>下面我们一个个的理解它们。</p><h2 id="static-cast"><a href="#static-cast" class="headerlink" title="static_cast"></a>static_cast</h2><p>Static_cast是在C++中最常用的casting方法。它可以用来在不同相关类型间做转换，例如数值类型（int，float）或者在父子类中转换。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a = <span class="number">3</span>;</span><br><span class="line"><span class="type">float</span> b = <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(a);</span><br></pre></td></tr></table></figure><p>上面是简单的数值类型转换，替换了之前的隐式转换。</p><p>下面通过一个例子说明class类型。</p><p>假设有一个父类class animal，两个继承animal的子类dog和cat，代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animal</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">makeSound</span><span class="params">()</span> </span>&#123; std::cout &lt;&lt; <span class="string">&quot;The animal makes a sound\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Dog</span> : <span class="keyword">public</span> Animal &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">makeSound</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; std::cout &lt;&lt; <span class="string">&quot;The dog barks\n&quot;</span>; &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">wagTail</span><span class="params">()</span> </span>&#123; std::cout &lt;&lt; <span class="string">&quot;The dog wags its tail\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Cat</span> : <span class="keyword">public</span> Animal &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">makeSound</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; std::cout &lt;&lt; <span class="string">&quot;The cat meows\n&quot;</span>; &#125;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">purr</span><span class="params">()</span> </span>&#123; std::cout &lt;&lt; <span class="string">&quot;The cat purrs\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>现在我们来看看如何使用<code>static_class</code>。</p><ol><li><p>Upcasting：Upcasting是将指向子类的指针转化为指向父类的指针，这样的操作在C++中是认为安全的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dog* dog = <span class="keyword">new</span> <span class="built_in">Dog</span>();</span><br><span class="line">Animal* animal = <span class="built_in">static_cast</span>&lt;Animal*&gt;(dog);  <span class="comment">// Upcasting</span></span><br><span class="line">animal-&gt;<span class="built_in">makeSound</span>();  <span class="comment">// Outputs: &quot;The dog barks&quot;</span></span><br></pre></td></tr></table></figure></li></ol><ol><li><p>Downcasting：Downcasting相反，将指向父类的指针指向子类。这是一种不安全的操作，因为父类的object可能并不是子类的object，这时候调用子类的object就会出现Undefined behavior。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Animal* animal = <span class="keyword">new</span> <span class="built_in">Animal</span>();</span><br><span class="line">Dog* dog = <span class="built_in">static_cast</span>&lt;Dog*&gt;(animal);  <span class="comment">// Unsafe downcasting</span></span><br><span class="line">dog-&gt;<span class="built_in">makeSound</span>();  <span class="comment">// Undefined behavior!</span></span><br></pre></td></tr></table></figure><p>安全的downcasting是这样的：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Animal* animal = <span class="keyword">new</span> <span class="built_in">Dog</span>();  <span class="comment">// The Animal is actually a Dog</span></span><br><span class="line">Dog* dog = <span class="built_in">static_cast</span>&lt;Dog*&gt;(animal);  <span class="comment">// Safe downcasting</span></span><br><span class="line">dog-&gt;<span class="built_in">makeSound</span>();  <span class="comment">// Outputs: &quot;The dog barks&quot;</span></span><br><span class="line">dog-&gt;<span class="built_in">wagTail</span>();  <span class="comment">// Outputs: &quot;The dog wags its tail&quot;</span></span><br></pre></td></tr></table></figure><p>这个例子中，animal变量本质就是Dog，因此downcasting为Dog类型就可以使用对应的函数。</p></li></ol><h2 id="dynamic-cast"><a href="#dynamic-cast" class="headerlink" title="dynamic_cast"></a>dynamic_cast</h2><p><code>dynamic_cast</code>主要用于类的downcasting和upcasting。这种cast会在runtime动态检查，在upcasting时，这种是更加安全的操作，效果与<code>static_cast</code>类似。但是在downcasting时，<code>dynamic_cast</code>会对其进行安全检查，这时后会访问父类的虚函数表（只有父类定义了虚函数才有虚函数表，有虚函数说明具有父类转换子类的需求和能力），如果没有虚函数表或没找到相关的信息，或者object不是目标类型，动态检查会失败，即返回一个nullptr。</p><p><strong>Note</strong>：要使用<code>dynamic_cast</code>，且downcasting时，必须父类拥有虚函数。</p><p>下面是使用<code>dynamic_cast</code>的例子：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;&#125;  <span class="comment">// Making this class polymorphic</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Base* basePtr = <span class="keyword">new</span> <span class="built_in">Derived</span>();</span><br><span class="line">Derived* derivedPtr = <span class="built_in">dynamic_cast</span>&lt;Derived*&gt;(basePtr);</span><br><span class="line"><span class="keyword">if</span> (derivedPtr) &#123;</span><br><span class="line">    derivedPtr-&gt;<span class="built_in">bar</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个例子中，Base是基类且有虚函数，Derived是派生类。</p><h2 id="reinterpret-cast"><a href="#reinterpret-cast" class="headerlink" title="reinterpret_cast"></a>reinterpret_cast</h2><p>Reinterpret_cast不会在cast过程中进行安全检查，需要程序员在使用的时候格外小心。理解起来较为方便，首先把某个数据或者object当作内存里的一块01组成的block。接下来，在做reinterpret cast的时候，本质就是在这个block之上加入不同的看待这个block的视角。我可以cast为int，那么使用的时候就被解释为int，我可以cast到一个class，那么使用的时候就被解释为一个class。</p><p>给予最大的自由，不加繁琐的限制，至于安全问题交给程序员来操心。</p><h2 id="const-cast"><a href="#const-cast" class="headerlink" title="const_cast"></a>const_cast</h2><p><strong>🚧(building…)🚧</strong></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BatchNorm vs LayerNorm</title>
      <link href="/2023/06/13/BatchNorm-LayerNorm/"/>
      <url>/2023/06/13/BatchNorm-LayerNorm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近在复习之前学过的Transformer的底层原理，又一次碰到了LayerNorm。之前在做一些深度估计项目的时候，使用ViT架构，也用到了LayerNorm。但仅仅将它作为了一个黑箱，并没有深刻的理解它的原理。而且与LayerNorm对应的BatchNorm，虽然之前阅读过原论文，但仍处于一知半解的阶段，大部分也忘记了。借助这个blog，重新学习和回忆一下。并对比两者的不同，并动手实现相应的模块。</p></blockquote><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>训练一个神经网络其实是比较困难的，因为神经网络的学习过程是<strong>反向传播（Back-propagation）</strong>：通过<strong>梯度下降（Gradient Descent）</strong>的方式一步步反向更新每一个神经元的权重。而且神经网络一般由多个层构成。这样就造成了一个问题，假设我们有A，B两层神经网络。A的输入是x，输出是y，B的输入是y，输出是z。计算与标签的损失，求梯度进行反向传播之后，A变成了A‘，B变成了B’。再向A输入x，得到的就再也不是y了，而可能是k。这时对于B而言，它之前所学到的关于y的信息就不适用了，必须重新进行adaptation。这样的现象叫做<strong>Internal covariate shift</strong>。这也是提出Batch Normalization的作者的动机。</p><p>但Santurkar等人的工作How Does Batch Normalization Help Optimization中指出，使得Batch Normalization成功的原因并非因为Internal covariate shift，甚至在某种情形下BatchNorm没有减少internal covariate shift。这篇blog不讨论谁的观点绝对正确，仅仅对BatchNorm的思想做总结。</p><h3 id="Why-Batch-Normalization"><a href="#Why-Batch-Normalization" class="headerlink" title="Why Batch Normalization"></a>Why Batch Normalization</h3><p><strong>一定程度消除Internal covariate shift</strong>：在introduction部分讲述了什么是internal covariate shift（ICS）。ICS导致每一层在更新参数后重新适应新的输入的变化，降低了学习的效率。采用BatchNorm，将输入归一化为均值为0和方差为1的数据，不仅仅可以一定程度解决梯度消失问题（不归一化数值会进入saturated zone，就是非线性激活函数的两端），还能使不同层之间近乎于独立学习，降低了层与层之间的耦合性。</p><p><strong>BatchNorm的平滑效应</strong>：BatchNorm将优化问题的landscape从“很瘦长的椭圆”转换为更平滑和对称的正圆（提升了损失函数的Lipschitzness和其梯度的Lipschitzness，关于Lipschitzness还在补充）。确保了问题的平滑性，就减少了初始值和学习率对神经网络的影响。我们可以采用更大的学习率加速网络学习，不必太过担心网络进入局部极小值。</p><h3 id="Mathematical-Description"><a href="#Mathematical-Description" class="headerlink" title="Mathematical Description"></a>Mathematical Description</h3><p>假设对一个layer的输入是$\vec x = (x^{ (1) } … x^{ (d) })$，那么对这个输入的归一化如下：</p><script type="math/tex; mode=display">\hat{x}_i ^{ (k) } = \frac{x_i^{ (k) } - E[x^{ (k) } ]} {\sqrt{Var[x ^{ (k) } ] + \epsilon}}</script><p>其中</p><script type="math/tex; mode=display">E[x^{ (k) } ] = \frac{1} {m} \sum_{i=1} ^{m} x_i ^{ (k) }</script><script type="math/tex; mode=display">Var[x^{ (k) } ] = \frac{1} {m} \sum_{i=1} ^{m} ( x_i ^{ (k) } - E[x^{ (k) } ] )</script><p>$m$表示mini-batch的batch size，$k$表示输入的第几个特征。</p><p>但如果就这样硬生生将每一层的activation后的输出归一化的话，会让模型丧失原有的表达能力。以下是ChatGPT的回答：</p><blockquote><p>The linear transformation process in Batch Normalization, which involves the learnable parameters (scale factor $\gamma$ and shift $\beta$), is crucial for the following reasons:</p><ol><li><p><strong>Restoring the Representational Power of the Network</strong>: After applying Batch Normalization, the activations of the layer are normalized to have zero mean and unit variance. While this normalization process helps to stabilize learning and reduce internal covariate shift, it can also limit what the layer can represent. For instance, in some cases, the network might learn that the best representation of the data for the subsequent layers is not zero-mean/unit-variance. The scale and shift transformation allows the network to learn the most suitable scale and location of the activations, thereby restoring the representational power of the network.</p></li><li><p><strong>Preserving the Expressive Power of Activation Functions</strong>: Certain activation functions like ReLU and its variants have different behaviors in different regions of the input space. For instance, the ReLU function is sensitive to positive inputs and insensitive to negative inputs. If Batch Normalization is used without the scale and shift, the activations would be mostly confined to the region where ReLU is active, thereby limiting the expressive power of the activation function. The scale and shift transformation allows the network to learn to use the full expressive power of the activation function.</p></li><li><p><strong>Flexibility</strong>: The learnable parameters $\gamma$ and $\beta$ provide the network with the flexibility to learn the optimal scale and mean of the activations. If the optimal scale and mean are indeed 1 and 0 respectively, the network can learn γ close to 1 and β close to 0. But if they are not, the network has the flexibility to learn other values.</p></li></ol><p>In summary, the linear transformation process in Batch Normalization, governed by the learnable parameters $\gamma$ and $\beta$, is crucial for preserving the expressive power of the network and providing it with the flexibility to learn the most suitable representations.</p></blockquote><p>因此在归一化之后，需要加入两个可学习的参数，给予模型自由学习的能力。让其在学习的过程中自己寻找最适合的分布状态。其数学描述为：</p><script type="math/tex; mode=display">y_i ^{ (k) } = \gamma^{ (k) } \hat{x}_i ^{ (k) } + \beta ^ { (k) }</script><p>在训练阶段，BatchNorm按照上述的数学描述进行学习。</p><p>但在验证和测试阶段，BatchNorm则有所不同。因为在训练阶段，我们是以mini-batch的形式将数据喂入模型的。训练过程中，会实时计算mini-batch的均值和方差。而如果在测试阶段也这样做，尤其是每次输入一个测试数据的时候，mini-batch的size是1，最后会得0。这显然是不合理的。因此在工程上，采用running mean和running variance的方法，会在训练阶段实时更新。</p><p>更新过程为：</p><script type="math/tex; mode=display">E[x^{ (k) } ]' = momentum \times E[x^{ (k) } ]' + (1 - momentum) \times E[x^{ (k) } ]</script><script type="math/tex; mode=display">Var[x^{ (k) } ]' = momentum \times Var[x^{ (k) } ]' + (1 - momentum) \times Var[x^{ (k) } ]</script><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>加速神经网络训练的方法可以用之前提到的batch normalization。但是有两点需要注意：</p><ul><li>Batch normalization比较依赖于mini-batch的大小，mini-batch越大，batch norm的效果越好</li><li>Batch normalization似乎对于RNN这种处理sequence数据的模型适用难度较高</li></ul><p>基于这两点因素，作者提出了Layer Normalization。</p><h3 id="Why-Layer-Normalization"><a href="#Why-Layer-Normalization" class="headerlink" title="Why Layer Normalization"></a>Why Layer Normalization</h3><ul><li>因为Batch normalization使用的方差和均值是基于mini-batch对整体的估计，这说明其受限于mini-batch的大小。</li><li>Batch normalization在序列模型中不太适用，因为序列模型的输入经常是变长的。</li></ul><h3 id="Mathematical-Description-1"><a href="#Mathematical-Description-1" class="headerlink" title="Mathematical Description"></a>Mathematical Description</h3><p>假设对一个layer的输入是$\vec x = (x^{ (1) } … x^{ (d) })$，那么对这个输入的归一化如下：</p><script type="math/tex; mode=display">\hat{x} ^{ (k) } = \frac{x ^{ (k) } - E[\vec x]} {\sqrt{Var[\vec x] + \epsilon}}</script><p>其中</p><script type="math/tex; mode=display">E[\vec x] = \frac{1} {d} \sum_{k = 1} ^ {d} x ^ { (k) }</script><script type="math/tex; mode=display">Var[\vec x] = \frac{1} {d} \sum_{k=1} ^{d} ( x ^{ (k) } - E[\vec x] )</script><p>为了保留模型的表达能力，还是加入两个可学习的参数做一个线性变换。</p><script type="math/tex; mode=display">y ^{ (k) } = \gamma^{ (k) } \hat{x} ^{ (k) } + \beta ^ { (k) }</script><p>Layer Normalization的训练和测试阶段的行为一直，因此不需要额外加入其他变量来做记录。仅需要在推理的时候，计算输入数据的均值和方差就可以。</p><h3 id="Batch-vs-Layer"><a href="#Batch-vs-Layer" class="headerlink" title="Batch vs Layer"></a>Batch vs Layer</h3><ul><li>Batch normalization受mini-batch size的影响，size越小，计算出的batch statistics越不能代表整体。而Layer normalization与batch的大小是独立的。</li><li>Layer normalization更适合用于sequence model，处理变长数据。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Batch Normalizaiton: Accelerating Deep Network Training by Reducing Internal Covariate Shift</li><li>How Does Batch Normalization Help Optimization</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning Optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Life Blog - Trip to Qingdao</title>
      <link href="/2023/06/11/Life-001/"/>
      <url>/2023/06/11/Life-001/</url>
      
        <content type="html"><![CDATA[<blockquote><p>因为疫情的原因，19年上大学的我整整三年没有怎么出去旅行。大四下学期是一个比较惬意的时候，没有繁琐的课程、需要参加的活动等等，时间相对空闲。也算是弥补一下前三年的遗憾，赶着毕业前，去一趟青岛和蓬莱。</p></blockquote><h2 id="01-前言"><a href="#01-前言" class="headerlink" title="01 前言"></a>01 前言</h2><p>还是花了一些时间规划的。对于像我这样的穷游党而言，主要的开销就是出行和住宿。刚开始制定的旅游计划相当极限，青岛、烟台、蓬莱和济南要在五天四夜之间全部游览完。有两天晚上都需要在火车上度过，尤其是返程上海时，凌晨2点的普快，一直要坐到第二天12点… 来回往复并算上中间所有旅游地点的车费，大约需要1.5k。</p><p>后来听取了一些同学的建议，去掉了济南的行程。在青岛呆两天半，第四天早上青岛北站出发到蓬莱，当天下午六点从蓬莱出发到烟台。烟台游玩一天，第六天返程。出行所需的费用被优化到了700块。大成功哈哈哈。</p><p>自己非常喜欢旅游，因为小时候旅游带给我的感觉是全身心的放松。处于与自己生活环境截然不同的地方，暂时抛去学习和日常生活，去短暂感受没有任何枷锁束缚的舒畅。</p><h2 id="02-青岛"><a href="#02-青岛" class="headerlink" title="02 青岛"></a>02 青岛</h2><h3 id="Day-1"><a href="#Day-1" class="headerlink" title="Day 1"></a>Day 1</h3><p>去了青岛的第一天已经到了下午了，由于比较累，直接在住的酒店睡了一下午。当天没有任何行程。</p><h3 id="Day-2"><a href="#Day-2" class="headerlink" title="Day 2"></a>Day 2</h3><p>当然在第一天也不是什么都没干。在床上躺着的时间整理了一下需要去的景点。整理景点的时候才发现，订酒店订的位置在青岛市市北… 而青岛几乎所有景点都在市南的沿海。</p><p>旅游的路线是从西向东规划的。第一天要去的地方是：</p><ul><li>栈桥</li><li>海军博物馆</li><li>小青岛公园</li><li>小鱼山公园</li></ul><p>栈桥就是在青岛市西南侧的一个小角落，栈桥右侧是布满礁石的海滩，而左侧是细石沙。它的尽头有立着回澜阁。通过照片能看出来今天不适合旅游，肉眼可见的大雾，直接压在头顶上，把每一个高楼的头部擦除。</p><p><img src="qd1.png" alt=""></p><p>中午去吃了一碗面，20元…，没吃饱又点了一个夹肉馍，说实话不太好吃，但要10块。一顿饭30，让我第一次觉得青岛的物价也可以赶超上海（有可能是景点附近缘故？）。吃饭的附近还有个church。</p><p>从吃饭的地方坐几分钟公交就到了海军博物馆，去里面浏览了一些海军的历史和展出的物品，也去海军博物馆后面的两艘舰上绕了一圈。从舰上可以看到小青岛公园。不得不说沿海各个景点离的是真近。</p><p><img src="qd2.png" alt=""></p><p>小青岛绕着海边走了走，海浪拍击在石头上的声音很清脆，海风也很大。走到海边会感觉很冷，走到不靠海的地方又马上热了起来。</p><p><img src="qd3.png" alt=""></p><p>下午快四五点到了小鱼山山顶，据说这里可以一览无余青岛市，可以欣赏大海，也可以看城市的建筑风格。这时，大雾的伏笔显现了…</p><p><img src="qd4.png" alt=""></p><p>晚上我去见了在青岛上学的同学，一起吃了一顿烧烤后去五四广场和奥帆中心看了一眼。这个时候的大雾沉到地面上，和下雨一样，走了一圈直接浑身湿掉了。</p><p><img src="qd5.png" alt=""></p><p>完全什么都看不见。</p><p>End of this day</p>]]></content>
      
      
      
        <tags>
            
            <tag> Life Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Workflow in the era of LLM</title>
      <link href="/2023/05/27/LLM/"/>
      <url>/2023/05/27/LLM/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在大语言模型时代，我们的学习和工作方式都有了很大的变化。依赖于生成式AI的强大能力，当前的LLM已经成为了各种APP、甚至操作系统的一部分。ChatGPT以及new bing的出现，展现出LLM的可能性。在写代码、学习新知识、撰写文档、归纳总结、阅读文献等众多任务中，可以成为很智能的Copilot。</p><p>之前学习的流程十分的复杂，东一块西一块，被各种所谓的“生产力”APP迷的眼花缭乱。我觉得也是时候总结一个对我而言高效的学习流程了。这篇博客主要以我为视角，介绍了我的生产力模型。</p></blockquote><h2 id="Study-Flow"><a href="#Study-Flow" class="headerlink" title="Study Flow"></a>Study Flow</h2><p><img src="Flow.png" alt=""></p><p>StudyFlow主要分为四个部分：Collector，Processor，Maintainer以及Scheduler。下面首先对各个部分做介绍，然后再整体叙述整个流程。</p><h3 id="Collector"><a href="#Collector" class="headerlink" title="Collector"></a>Collector</h3><p>这一部分是收集器，收集信息来源的两个主要方面就是利用Google搜索引擎和New bing搜索引擎。首先对于一些想要了解的问题，可以先咨询New bing，让其对网络的信息进行过滤，对提供的网页链接快速浏览。也可以用Google和new bing作为纯搜索引擎。</p><p>Collector中检索出的信息分为三类：</p><ul><li>流媒体视频信息：包括自媒体视频、网络公开课。此类信息是经过他人理解转述的信息。理解难度低，信息准确度不高。</li><li>博客、文章信息：是相对于视频动态信息的静态信息。也是经过他人理解转述的信息。理解难度低，但信息准确度不高。</li><li>Paper、书籍、代码信息：相对于前两者，此类信息更权威、更精确和精准。理解难度高，信息准确度高。</li></ul><p>针对前两类信息（流媒体和博客），由于理解难度低，在深入学习一个问题之前，可以先检索类似的信息。通过观看视频和阅读博客，对将要学习的内容建立起感性全局的认知，这些信息可以先缓存到Maintainer的ICU（Information Cache Unit）中，在每日的总结和回顾中将有用的信息维护进KMU）。</p><p>对于Paper、书籍、代码类的信息：该类信息理解难度高，处理起来费时费力。保存到icloud中，便于跨平台检索。</p><h3 id="Processor"><a href="#Processor" class="headerlink" title="Processor"></a>Processor</h3><p>信息处理器。主要靠自己的理解和ChatGPT的辅助。对于ChatGPT而言，在编程和相关技术领域的回答还是比较正确的，当然不排除胡诌的可能性，需要通过实验和对比网上的问题进一步验证。遇到一些细节问题，可以将ChatGPT作为一个耐心的助手来询问，尽管有时候回答是错误的，但也能启发一些理解的思路，说出来问题和他人对话，比自己一个人想的效果在某些情况下要好很多。</p><p>对于流媒体和文章类的信息，需要自己理解将这些内容归纳和总结。</p><p>对于Paper和书籍类的信息，则需要用一些处理工具（当然，文章类和媒体类也可以用，主要是帮助自己理解）。</p><p>MarginNote3 十分擅长创建一本书的思维导图，并提供了便捷的跳转功能。可以将整本电子书的内容结构化，在回顾思维导图的时候，可以点击每一块来快速访问原文。这个软件可以处理书籍。</p><p>但是，人们总是喜欢写写画画。在ipad的上，MarginNote对于书写的体验并不是很好。GoodNotes可以说是书写体验极佳的一款笔记软件。可以用它处理Paper，在Paper上勾画做笔记。</p><p>这两款软件的资源来自于icloud，icloud是在Collector阶段获取的。</p><p>icloud与苹果的搜索是联动的，而且搜索速度也很快。如果想看一个文章的笔记还要再打开GoodNotes等，相当麻烦。可以采用的策略就是把做好笔记的Paper覆盖原有的内容。</p><h3 id="Maintainer"><a href="#Maintainer" class="headerlink" title="Maintainer"></a>Maintainer</h3><p>维护器的本质就是存储信息和数据的数据库。这里采用的是Notion作为数据管理中心。原因是它可以在任何平台访问，包括在Web端，没有硬件限制，随时随地取用和检索。</p><p>我在Notion创建KMU和ICU两个page。KMU负责结构化的组织和管理笔记和知识，需要使用者主动的输出和维护。ICU则是快速缓存每天的Idea和获取的原始信息（raw info），需要周期性的被processor处理和固化到KMU才能变成有用的信息。</p><h3 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h3><p>调度器的工作是最重要的。当把信息处理好并固化到你的Maintainer当中，放在那里不动是不管用的。你需要对相关信息即使的更新维护和复习，加深对这些东西的理解，反复提高KMU内容的可理解性和可阅读性。</p><p>我经常遇到的问题是：当我学习某个领域的信息，大脑在这个语境之下，理解知识后，写笔记的过程中就会主动忽略掉某些自认为已经清晰的细节，导致写出来的笔记和内容并不连贯。这就导致，当一段时间没复习时，再回顾这个笔记，有很多地方读起来不通顺，并且很多细节又不理解了，还要花大量时间重新理解和推导。</p><p>解决这个问题的方法有两个：</p><ul><li>定期的复习，始终让大脑熟悉这个领域的语境下</li><li>周期性的维护，提升笔记阅读的流畅性和可理解性</li></ul><p>这就需要一个调度器来管理日常的复习和任务（当然，最最重要的还是个人的执行力）。</p><p>调度器使用的是苹果的calendar，可以实现iphone、watch、mac之间的同步提醒功能。我把它当作个人的日常规划软件，主要是指定计划和提醒。</p><h2 id="Project-Flow"><a href="#Project-Flow" class="headerlink" title="Project Flow"></a>Project Flow</h2><p>Project包括电子设计项目、大型程序项目、难度高的编程任务以及课程Lab等。这类Project的特点是复杂度高，需要很强的分层设计来降低任务的复杂度。如果一头黑的去完成这类任务时，成功率很低，即使成功，做出来的项目的可维护性、可读性和可扩展性也绝对不高。</p><p>因此，在开始任务前，对任务建立一个high-level角度的理解，再通过细粒度的划分层级结构，将抽象思想融合进去，能够全方面提高项目的质量和成功率。</p><p>同时，设置相关的Deadline，不断推进任务也至关重要。</p><h3 id="High-level-Understanding"><a href="#High-level-Understanding" class="headerlink" title="High-level Understanding"></a>High-level Understanding</h3><p>对一个技术或项目的高层理解，建立在直觉上。可以用通俗易懂的语言去描述它是这个阶段的目标。这也是学习的过程，因此可以采用Study Flow的流程，首先扫去知识盲区。</p><p>扫去知识盲区之后，下一步就是通过画草图的形式，将概念和架构可视化，清晰的表述不同组件和模块之间的关系。</p><h3 id="Divide-and-Conquer-amp-amp-Abstract"><a href="#Divide-and-Conquer-amp-amp-Abstract" class="headerlink" title="Divide and Conquer &amp;&amp; Abstract"></a>Divide and Conquer &amp;&amp; Abstract</h3><p>分而治之和抽象是解决复杂任务的关键。在设计系统时，首先仔细思考如果将目标抽象、并划分为不同组件；进一步考虑每个组件之间的关系是什么，为了实现high-level的效果，如何分配每个组件的功能。</p><h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><p>这是最后一步，采用各种不同的技术和数据结构、算法，实现每一个功能。</p><p>在project开始时，首先要从coarse-grain和fine-grain的角度理解系统，general overview的直觉要有，技术细节也要有。尝试用抽象的方式，对需要的对象抽象，并探索抽象出的组件之间的关系。</p><p>之后就是项目的实时推进过程，从项目层面进行任务划分，和时间上进行任务划分（因为不可能一天就写完项目）。每天完成分配好的项目后，因为面对的是一个复杂系统，要对当前所完成的功能和在全局视野中的位置做一个标定，意识到自己在哪，该干什么，然后对明日的任务或之后的任务进一步划分。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Study </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU 15-445 Buffer Pool Manager</title>
      <link href="/2023/04/01/Buffer_Pool_Manager/"/>
      <url>/2023/04/01/Buffer_Pool_Manager/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Buffer Pool Manager Instance这个Task本质而言就是理解清楚Extendible Hash Table和LRU-K Replacer的关系，然后将它们组合起来。实现的难度并不高，只要按照注释内容好好分析，然后按逻辑写下来就可以。重点在于每一个操作的细节问题，细节问题很多，要时刻注意。</p></blockquote><h2 id="Buffer-Pool-Manager-Mechanism"><a href="#Buffer-Pool-Manager-Mechanism" class="headerlink" title="Buffer Pool Manager Mechanism"></a>Buffer Pool Manager Mechanism</h2><p>想要实现Buffer Pool Manager，我们就要在high-level的角度理解一下Buffer Pool Manager在系统中的作用。先看下图：</p><p><img src="bpm1.png" alt=""></p><p>站在System的角度看，System会向Buffer Pool Manager发送一个请求，指定一个page id，Buffer Pool Manager返回这个page在内存中的地址（或者新建一个page）。仅此而已，系统只需要关心要那个page，并等待Buffer Pool Manager返回即可。</p><p>接下来我们去BPM内部看一下。当System想要Fetch一个page时，为了快速在内存中找到指定page id的位置，这里用了Extendible Hash Table。根据之前实现的Hash Table，执行Find操作，将对应的位置，也就是frame id查询出来（<strong>因为Buffer Pool Manager在内存中创建page slot的时候是以数组的形式创建的，因此frame id对应的就是数组的下标索引</strong>）。接着我们可以在内存中找到，并返回page的地址即可。</p><p>如果Fetch操作在哈希表中没找到对应的frame id，说明内存中并不存在这个page。接下来做的操作，就是想从Disk中将对应page的数据copy进来，同时在内存中给它找到一席之地。</p><p>从上图的Buffer Pool中可以看到，内存中灰色的page表示的是当前没有存放page的内存块。因此想从Disk中复制数据进来时，我们就可以先看看Free List中有无空闲的空间。如果有，就采用这个对应的frame id。如果没有，说明内存中所有的page全都放满了，那么就需要从LRU-K Replacer中，选取一个适合驱逐的内存块驱逐它，留出位置给从Disk copy的page使用。</p><h2 id="Buffer-Pool-Manager-Instance-Implementation"><a href="#Buffer-Pool-Manager-Instance-Implementation" class="headerlink" title="Buffer Pool Manager Instance Implementation"></a>Buffer Pool Manager Instance Implementation</h2><p>相关文件夹提供了API及其注释，分别有<code>NewPgImp</code>、<code>FetchPgImp</code>、<code>UnpinPgImp</code>、<code>FlushPgImp</code>、<code>FlushAllPgsImp</code>以及<code>DeletePgImp</code>需要我们实现。相关实现需要注意的地方注释中写的很清晰，当然还有很多细节部分注释中并没有标注出来。比如哈希表的增加和删除的时机、pin count增加的时机等等，这些都是需要自己去debug过程中发现代码漏洞而去补全的。</p><p>这里主要说明一下数据结构部分。<code>pages_</code>是一个page数组的首地址，既然是数组，那说明它们的内存空间是连续的，而且支持随机查找。<code>pages_</code>的索引就是frame id，表明这是buffer pool中第几个slot。<code>disk_manager_</code>我们只需要用到其中的<code>WritePage</code>和<code>ReadPage</code>的API即可。<code>page_table_</code>实现的是page id到frame id的映射关系，使得对指定page id的查找时间复杂度从O(N)降到了O(1)。使用这个数据结构的时候，要注意删除一个page时随即删除<code>page_table_</code>的相关记录，不然之后会导致读到的page的内容是不同page id内容的情况。<code>replacer_</code>的作用是，如果当前buffer pool全部满了，提供一个可驱逐的frame id号，buffer pool manager会根据这个frame id对这块内存进行驱逐，再把想要的page从Disk拷贝到这块内存中。</p><p>关于线程安全，我还是一把大锁锁住所有，尝试过优化，但并没有明显的效果。Leaderboard的结果在2.1s左右。希望各位大佬能对优化方案提供一些建议。</p><p>附一张截图：</p><p><img src="bpm2.png" alt=""></p><h3 id="踩坑记录"><a href="#踩坑记录" class="headerlink" title="踩坑记录"></a>踩坑记录</h3><ul><li><code>FlushPgImp</code>并不是要将指定page重写回disk并刷新这块内存空间的意思，单纯就是为了将这page写会Disk刷新一下，不改变其在内存中的状态。<code>FlushAllPgsImp</code>同理。</li><li>每次<code>FetchPgImp</code>，就要增加指定page中的pin count。</li><li>Evict和Remove一个page之后，不要忘记<code>ResetMemory</code>。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Database Management System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU 15-445 LRU-K Replacement Policy</title>
      <link href="/2023/03/26/LRU-K_Replacement_Policy/"/>
      <url>/2023/03/26/LRU-K_Replacement_Policy/</url>
      
        <content type="html"><![CDATA[<blockquote><p>实现LRU-K的时候，先去完成了LeetCode的146题LRU，实现的时候运用了哈希链表结构，题目中要求get和put操作都是O(1)的时间复杂度。一种实现方式就是用STL的list和unordered map，list本质是双向链表，但实现出来最后的结果总是不尽人意。因此将list结构换成自己实现的双向链表，运行起来的速度和内存都优于list的实现。因此在15-445的LRU的实现中，我也采用了自己实现的一个内嵌结构体作为双向链表的基础。总体而言，参考了网上对于LRU-K的讲解之后，对LRU-K的算法操作还是比较清晰的，不像Extendible Hash Table那样一知半解。所以实现起来也很清晰。但由于用的是自己写的双向链表，debug花时间最久的就是内存泄漏问题，因为之前写过的一些项目都没有考虑，因此花了很长时间去定位错误。总体来说是一个教训。</p></blockquote><h2 id="Replacement-Policy"><a href="#Replacement-Policy" class="headerlink" title="Replacement Policy"></a>Replacement Policy</h2><p>缓存驱逐算法的应用场景是，由于内存和硬盘之间读写速度的巨大差异，并且有程序局部性原理的存在，我们想通过<strong>保存从硬盘中读取出的东西在内存中</strong>，这样一种方法来尽可能减少两者之间速度的差异，提高CPU的利用率和处理速度，减少不必要的等待时间。从硬盘中读取出的数据一定是换存在内存中的，而由于内存的大小局限性，我们不可能无限的存储硬盘的page，甚至只能分配内存的一小部分作为缓存空间来使用。</p><p>这样的缓存空间一定是有大小的，有大小就会涉及到缓存存满的问题。如果想要再存放新读入的数据到缓存空间中，必须选择一个内存块将其驱逐出去。把空间给腾出来。选择哪一块驱逐，就是替换算法（replacement policy）所做的事情。一般常见的替换策略有FIFO，LRU以及LFU等等。</p><p>下图展现了硬盘数据和内存之间提取（Fetch）和驱逐（Evict）的关系。</p><p><img src="lruk1.png" alt=""></p><p>上面图片中，Buffer Pool就是缓存数据的池子，最大容量为3个page。Disk上面的Data数不胜数，当需要数据时系统会从Buffer Pool检索，如果检索到了，则直接从内存中提出数据，并且通知<strong>LRU-K Replacement Policy</strong>组件调用了这个page。如果没检索到，就从硬盘中读取一块。如果此时Buffer Pool满了，那就询问<strong>LRU-K Replacement Policy</strong>组件，索要一个可以驱逐的frame id号，系统再根据这个frame id从内存中把对应的page重新写回硬盘（<strong>NOTE：LRU-K不做任何实际的数据操作，仅仅是记录frame的状态，并根据一定的策略将可以驱逐的frame id号提供给系统</strong>）。</p><h2 id="LRU-and-LRU-K"><a href="#LRU-and-LRU-K" class="headerlink" title="LRU and LRU-K"></a>LRU and LRU-K</h2><p>建议还是去做一下力扣146题，通过这道题可以了解LRU的算法流程。LRU其实很简单，用语言表述一句话就是：选择最长时间不访问的page驱逐。话是这么说，实现上还是需要一些技巧的。题目中需要实现两个方法，一个是get一个是put。put是给定一个key，value，将其存入链表中。get操作是给定key，返回一个key对应的value，并删除对应的key/value pair。题目要求两个操作的时间复杂度为O(1)。</p><p>链表的插入和删除操作的复杂度为O(1)，但是查找的时间复杂度为O(N)。这就没办法满足get操作的要求。我第一次实现的时候直接用了C++的list容器，结果最后超时了。如果查找满足时间复杂度为O(1)，就需要用哈希数据结构。但哈希表是无序结构，LRU需要有序结构。结合两者的优点，就是哈希链表。查找时索引哈希表，得到对应链表的地址，插入和删除时直接用双向链表的删除即可。</p><p>用例子来理解一下LRU的算法。假设LRU最大存储量为3，接下来对算法进行put(1, 1), put(2, 2), put(3, 3), get(2, 2), put(1, 2)的操作。看下图的操作顺序（<strong>我实现的方式是，链表头是要驱逐的元素，链表尾部是最新使用过的元素</strong>）。</p><p><img src="lruk2.png" alt=""></p><p>当然这只是双向链表的视角，别忘了，为了实现O(1)的get操作，我们还有一个哈希结构。下面是一个哈希链表结构的示意图。</p><p><img src="lruk3.png" alt=""></p><p>新增的头节点L和尾节点R可以用来直接对头部和尾部进行操作，头部要移除，尾部要插入，就不必去寻找头尾节点了。这两个头尾节点在之后的LRU-K算法实现中会扩展成头部、中间和尾部节点，关于这部分之后在叙述我的想法。</p><p>Leetcode的代码实现如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_map&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">        <span class="type">int</span> key_;</span><br><span class="line">        <span class="type">int</span> val_;</span><br><span class="line">        Node* left = <span class="literal">nullptr</span>;</span><br><span class="line">        Node* right = <span class="literal">nullptr</span>;</span><br><span class="line">        <span class="built_in">Node</span>(<span class="type">int</span> k, <span class="type">int</span> v) : <span class="built_in">key_</span>(k), <span class="built_in">val_</span>(v), <span class="built_in">left</span>(<span class="literal">nullptr</span>), <span class="built_in">right</span>(<span class="literal">nullptr</span>) &#123;&#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="type">int</span> size;</span><br><span class="line">    <span class="type">int</span> capacity_;</span><br><span class="line">    std::unordered_map&lt;<span class="type">int</span>, Node*&gt; cache_map_;</span><br><span class="line">    Node *R;</span><br><span class="line">    Node *L;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">LRUCache</span>(<span class="type">int</span> capacity) : <span class="built_in">capacity_</span>(capacity) &#123;</span><br><span class="line">        R = <span class="keyword">new</span> <span class="built_in">Node</span>(<span class="number">-1</span>, <span class="number">-1</span>);</span><br><span class="line">        L = <span class="keyword">new</span> <span class="built_in">Node</span>(<span class="number">-1</span>, <span class="number">-1</span>);</span><br><span class="line">        R-&gt;right = L;</span><br><span class="line">        R-&gt;left = L;</span><br><span class="line">        L-&gt;right = R;</span><br><span class="line">        L-&gt;left = R;</span><br><span class="line">        size = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">insertNode</span><span class="params">(Node *node)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// insert to back of list</span></span><br><span class="line">        node-&gt;left = R-&gt;left;</span><br><span class="line">        node-&gt;right = R;</span><br><span class="line">        R-&gt;left-&gt;right = node;</span><br><span class="line">        R-&gt;left = node;</span><br><span class="line">        size++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">moveNodeToBack</span><span class="params">(Node *node)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">eraseNode</span>(node);</span><br><span class="line">        <span class="built_in">insertNode</span>(node);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">eraseNode</span><span class="params">(Node *node)</span> </span>&#123;</span><br><span class="line">        node-&gt;left-&gt;right = node-&gt;right;</span><br><span class="line">        node-&gt;right-&gt;left = node-&gt;left;</span><br><span class="line">        size--;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> it = cache_map_.<span class="built_in">find</span>(key);</span><br><span class="line">        <span class="keyword">if</span> (it != cache_map_.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            <span class="built_in">moveNodeToBack</span>(it-&gt;second);</span><br><span class="line">            <span class="keyword">return</span> it-&gt;second-&gt;val_;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">put</span><span class="params">(<span class="type">int</span> key, <span class="type">int</span> value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> it = cache_map_.<span class="built_in">find</span>(key);</span><br><span class="line">        <span class="keyword">if</span> (it != cache_map_.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            it-&gt;second-&gt;val_ = value;</span><br><span class="line">            <span class="built_in">moveNodeToBack</span>(it-&gt;second);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (size == capacity_) &#123;</span><br><span class="line">            cache_map_.<span class="built_in">erase</span>(L-&gt;right-&gt;key_);</span><br><span class="line">            <span class="built_in">eraseNode</span>(L-&gt;right);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">auto</span> node = <span class="keyword">new</span> <span class="built_in">Node</span>(key, value);</span><br><span class="line">        <span class="built_in">insertNode</span>(node);</span><br><span class="line">        cache_map_[key] = node;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>LRU-K算法是LRU算法的改进版。LRU算法存在的问题是，当存在大量的一次性操作时，会把历史的缓存冲刷掉，而新进入buffer pool的page有可能之后不会再访问了，被冲刷掉的page是之前保留下来的比较“有用”的page，这就是<strong>缓存污染</strong>问题。</p><p>LRU-K的思路是，永远最先驱逐访问次数小于K次的page。网上的很多讲解是直接维护两个链表，一个叫做history list，另一个叫buffer list。新加入的page总会先进入history list，当访问次数等于指定的次数K次时，就会从history list删除，并移动到buffer list的尾部（<strong>这里还是假设尾部的page是最新使用的，头部page是最近最久未使用的</strong>）。</p><p>通过例子来理解一下这个LRU-K算法，首先插入key=1，2，3，它们的value分别是1，1，1。原因是这里的value我用它来记录K的次数，也就是访问的次数。现在假设整个history list和buffer list最多存储三个page，就是buffer pool的大小，且K=2。首先我们访问key=1的page，那么对应的node的value就会变为2，一旦等于了K的值，说明这个page是可能被经常访问的，就把它移入buffer list。</p><p><img src="lruk4.png" alt=""></p><p>接着我们再访问key=2的page。</p><p><img src="lruk5.png" alt=""></p><p>Buffer list服从LRU算法，History List可以服从任意替换算法，在实验手册中，要求驱逐最早进入history list的page，采用的是FIFO策略（<strong>刚开始我实现的history list是LRU策略，结果会导致Evict Test无法通过，一定要注意history和buffer的策略不一样</strong>）。</p><p>接着我们尝试插入key=4的page，结果发现buffer pool满了（总共的空间为3，history占1，buffer占2），那么就需要对某一个page进行缓存驱逐，根据原则：<strong>永远最先驱逐访问次数小于K的page</strong>。所以先驱逐掉3，再加4到history list中。</p><p><img src="lruk6.png" alt=""></p><p>History list采用FIFO，由于History list只剩下3了，那么只能驱逐3。如果继续访问4，那么4会被移入Buffer list中，此时buffer pool也已经满了。如果想再插入一个新的page，需要缓存驱逐，这时就要从buffer list里面根据LRU算法进行驱逐了，具体的例子可以自行画一下。</p><p>以上就是LRU和LRU-K算法的解释。</p><h2 id="LRU-K-Implementation"><a href="#LRU-K-Implementation" class="headerlink" title="LRU-K Implementation"></a>LRU-K Implementation</h2><p>实现15-445 LRU-K任务的时候，我采用的还是使用自己定义的结构体Node和STL的unordered_map结构，实现哈希链表。链表不使用STL的list的一个原因是Leetcode跑下来的结果优于使用STL的list，性能方面会更好；另一个原因就是结构体定义数据和操作的自由度高，可以根据自己的想法来实现。缺点就是，<strong>要注意内存泄漏问题（由于之前经验很少，此处debug时间花的很多）</strong>。</p><p>实现思路总体继承Leetcode的解法，定义一个有key，value，左右指针的节点，同时为了区分每一个节点是否evictable，加如了一个bool变量。其中，key存储对应的frame_id，value代表被访问的次数，evictable表示这个节点是否可以被驱逐。</p><p>设计双链表的时候，除去左节点L和右节点R，我还加入了一个中间节点M，作为history list和buffer list的分割节点。两种list连接在一条双向链表上，再用unordered map索引。</p><p>具体的效果如下图：</p><p><img src="lruk7.png" alt=""></p><p>然后对history list和buffer list分别维护一个计数器，计算当前包含的节点数量，可用来判断是否需要驱逐缓存（注意需要迭代判断，并不能直接取L节点或M节点的下一个，因为下一个节点很有可能是non-evictable的）。</p><p>线程安全方面是直接一把大锁锁住所有操作。并没有细致的对每一个结构进行优化。</p><h3 id="踩坑记录"><a href="#踩坑记录" class="headerlink" title="踩坑记录"></a>踩坑记录</h3><ul><li><p>最主要的问题就是内存泄漏问题，真的是种种内存泄漏。最大的原因就是，程序结束之后，有一些内存中的节点，没有被人为的Remove或者Evict，导致结束程序后将这些节点遗留下来，造成的内存泄漏。解决方案就是在Replacer的析构函数里实现一个强制去除当前链表所有节点的程序（包括non-evictable，因为之前实现的是直接循环Evict，再删除L，M和R，但总还是有内存泄漏问题，原因在于Evict仅仅驱逐evictable数据，析构函数调用它会删不干净）。还有的内存泄漏问题就是use after free问题。这个问题值得是在释放内存空间后还尝试使用这个内存空间。经典出现的地方在于，删除节点的时候要伴随哈希表对应数据的删除。需要注意的是哈希表的<code>erase</code>方法：如果用key删除，且value存储的是指针，那么仅仅会删掉哈希表中的数据，指针所指向的内存是不会被删除的；如果用迭代器删除，我们首先应当删除迭代器的second所指向的内存空间，然后再删除迭代器的first对应的key。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// hash_map_&lt;int, Node*&gt;</span></span><br><span class="line"><span class="keyword">auto</span> it = hash_map_.<span class="built_in">find</span>(frame_id)；</span><br><span class="line">hash_map_.<span class="built_in">erase</span>(it-&gt;first);</span><br><span class="line"><span class="built_in">EraseNode</span>(it-&gt;second); <span class="comment">// 报错use after free，因为已经删除了对应的数据，迭代器不可用，it无法索引到</span></span><br><span class="line"></span><br><span class="line">Node *p = L-&gt;right; <span class="comment">// 取出头节点</span></span><br><span class="line">hash_map_.<span class="built_in">erase</span>(p-&gt;key)；</span><br><span class="line"><span class="built_in">EraseNode</span>(p)； <span class="comment">// 成功，因为hash map只删除了表中存储的数据，原Node内存还存在</span></span><br></pre></td></tr></table></figure></li><li><p>History list用的是FIFO，Buffer list采用LRU。我开始都用的LRU，导致EvictTest报错。</p></li><li>注意要实现自己的双向链表的删除，插入和移动的相关函数。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Database Management System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CMU 15-445 Extendible Hash Table</title>
      <link href="/2023/03/22/Extendible_Hash_Table/"/>
      <url>/2023/03/22/Extendible_Hash_Table/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近在学习CMU的15-445 DB课程，在做Project1的Extendible Hash Table的时候，由于是先看了课程，过了一个多星期才做的Lab，对extendible hash table只能说是知道大体的意思，并没有透彻的了解它，尤其是bucket指针和数据重分配这一部分，涉及到比较tricky的位运算，在一知半解的情况下实现它，完全没办法找到对应的bug，ConcurrentInsertFindTest和GetNumBucketsTest总是fail。又去参考了很多对可扩展哈希的文章，才发现自己一些细节是错误的。本篇文章尝试以我的理解说清楚extendible hash table，并作为我的菜坑记录。</p></blockquote><h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><p>这一部分的任务就是搭建一个通用的存储unique KV对的哈希表。我们需要实现哈希表的插入、删除以及查找操作。实验手册中并没有要求我们实现shrink部分，所以只需要关注如何扩展哈希表即可。代码在<code>scr/include/container/hash/extendible_hash_table.h</code>以及 <code>extendible_hash_table.cpp</code>下，实现之前建议先阅读这两个文件的代码和注释，明确我们的目标。</p><h2 id="Overview-of-Extendible-Hash-Table"><a href="#Overview-of-Extendible-Hash-Table" class="headerlink" title="Overview of Extendible Hash Table"></a>Overview of Extendible Hash Table</h2><p>在理解可扩展哈希表之前，我们需要了解几个概念。</p><ul><li><strong>Directory</strong>：是存放bucket指针的容器，可动态生长（以原大小的倍数作为增长率），容器的每个元素可用哈希值来索引。</li><li><strong>Bucket</strong>：桶。存放Key/value pair的桶，数据结构层面是一个线性表。</li></ul><p>下面是一个简单的可扩展哈希表的示意图，具体不用关心它是怎么来的，先对它建立一个直观的印象即可。</p><p><img src="eht1.png" alt=""></p><p>上图又出现两个概念：</p><ul><li><strong>Global Depth</strong>：假设global depth为n，那么当前的directory必定有$2^n$个entry。例如，当前$n=2$，那么就有4个entry，$n=3$就有8个entry。同时，给定一个key，需要用global depth取出这个key的低n位的二进制值。例如，一个key的二进制是10111，如果global depth是3，通过<code>IndexOf(key)</code>函数，得到返回值的二进制值是111，即为7。这个值用来索引directory[111]位置的bucket。</li><li><strong>Local Depth</strong>：local depth指的是（假设local depth为n），在当前的bucket之下，每个元素的key的低n位都是相同的。</li></ul><p>两者之间有什么关系呢？</p><ul><li>对于一个bucket来说，如果当前的global depth等于local depth，那说明这个bucket只有一个指针指向它。</li><li>如果当前的global depth大于local depth，必定不止一个指针指向它。</li><li>计算当前bucket有几个指针指向它的公示是$2^{globalDepth-localDepth}$。</li></ul><p>Global depth和local depth的概念就是这些，然而在实现算法的过程中还有对这些概念的应用，我们暂且先忽略，之后的部分会一一阐述。</p><h2 id="Implementation-Scheme"><a href="#Implementation-Scheme" class="headerlink" title="Implementation Scheme"></a>Implementation Scheme</h2><p>对于Bucket的Insert，Remove以及Find操作，熟悉一下C++的list容器相关操作就可以实现。不过有一个地方需要注意的是，实现bucket的Insert方法时，注释里说的是先检查key是否存在，如果存在就要更新value。这里如果先判断bucket是否满了，就会出现bug。因为如果一个bucket满了，但刚你要插入的key在这个bucket的中，先判断是否满的话就会直接返回，不会更新对应key的value，就会造成之后find的错误。</p><p>实现了Bucket的三个操作之后，就可以实现ExtendibleHashTable的三大操作了。为了确保线程安全，每一个操作应当加锁来保证。</p><p>这里阐述一下Insert的算法流程，然后结合一个具体的例子，分析算法可能遇到的情况。</p><ol><li>尝试插入Key，若插入成功，返回即可，若不成功，执行步骤2。</li><li>判断当前<code>IndexOf(key)</code>指向的bucket下，该bucket是否满了。如果满了，执行步骤3。否则执行步骤7。</li><li>如果当前global depth等于local depth，说明bucket已满，需要增长direcotry的大小。增加directory的global depth，并将新增加的entry链接到对应的bucket。否则，继续执行步骤4。</li><li>记录当前的local mask，创建bucket1和bucket2，增加两个bucket的local depth，增加num bucket的数量。取出之前满了的bucket中的元素，按照local mask的标准将每个元素重新分配到bucket1和bucket2中。执行步骤5。</li><li>对每个链接到产生overflow的bucket的direcotry entry，按照local mask的标准，重新分配指针指向。执行步骤6。</li><li>重新计算<code>IndexOf(key)</code>，执行步骤2。</li><li>插入指定的key/value pair。</li></ol><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>这个例子来自于官方的Homework #2 - Question 3。假定每一个bucket的容量大小为2，且哈希函数用最低的g个二进制位，g指global depth。</p><p>按顺序插入15，14，23，11，9。这几个数对应的二进制分别是，1111，1110，10111，1011，1001。</p><p><strong>STEP 1</strong>：首先插入15和14，第一步没什么问题。</p><p><img src="eht2.png" alt=""></p><p><strong>STEP 2</strong>：接着插入23，23的二进制是10111，当前的global depth是0，计算得到的<code>IndexOf(key)</code>是0，说明23要插入到directory的第0个entry中，但是这个entry所指向的bucket满了。我们执行步骤3（<strong><em>重复一下步骤3：如果当前global depth等于local depth，说明bucket已满，需要增长direcotry的大小。增加directory的global depth，并将新增加的entry链接到对应的bucket。否则，继续执行步骤4</em></strong>）。</p><p><img src="eht3.png" alt=""></p><p>这一步有一个很重要的点，新增长的entry怎么分配到对应的bucket？如果和上图的情况一样，从1增长到2，只需要把多出来的一个拉到唯一的bucket上就可以了，但如果从2到4，从4到8呢？多出来的若干个如何处理？其实只需要将多出来的一部分指针完全复制之前的一份就可以了。这样做法我觉得是可扩展哈希的比较重要的细节，由于可扩展哈希扩展direcotry时是按照当前大小的两倍进行扩展，新增长出来的部分作为之前directory的对等实体，每一个新的entry都对应了之前对应的entry，指向相同的bucket。唯一的不同就是之前的direcotry的索引最高位是0，扩展出来的最高位是1。</p><p><img src="eht4.png" alt=""></p><p><strong>STEP 3</strong>：执行步骤4（<strong><em>记录当前的local mask，创建bucket1和bucket2，增加两个bucket的local depth，增加num bucket的数量。取出之前满了的bucket中的元素，按照local mask的标准将每个元素重新分配到bucket1和bucket2中。执行步骤5</em></strong>）。</p><p>当前local mask的计算方法是<code>1 &lt;&lt; local_depth</code>，其中的local depth是指<strong>STEP 2</strong>图片中，扩展之前的local depth，即为0。</p><p>为什么呢？因为在扩展之前，产生overflow的bucket中的数据，低local depth个的二进制位完全相同，在<strong>STEP 2</strong>的图片例子中，1111和1110没有相同的低位二进制位，因此local depth是0。现在要插入23（0b10111），由于bucket已经满了，所以我们需要分裂bucket、重分配KV pair、重分配entry的指向。分裂了bucket，就产生两个bucket。</p><p>怎么放KV pair呢？我们总不能乱放吧？我们肯定要有规律的去分配。1111和1110，由于之前local depth为0，表明不需要参考任何二进制位，因此可以放到一个bucket里。当插入10111时，一个bucket放不下了，就需要两个bucket，为了可以高效的查询，当然是<strong>归类分配</strong>才行。按什么归类？当我们给事物归类的时候，我们会按属性归类，玩具为一类，家具为一类。二进制怎么归类呢？我们可以从最低位二进制位开始对比，之前不需要对比，现在我们至少需要对比一个二进制位，才能将3个二进制数分为两类（2个+1个，如果对比一个二进制位还不行，就继续增加local depth）。local mask的意思就是，之前local depth为0，不需要对比，但我现在要对比第一位，那么我就可以使用<code>1 &lt;&lt; local_depth</code>，1左移0位还是1，就是对比第一位二进制位。通过对比，1111和1110就不再是一类了，可以分别放入不同的bucket。</p><p><img src="eht5.png" alt=""></p><p><strong>STEP 4</strong>：执行步骤5（<strong><em>对每个链接到产生overflow的bucket的direcotry entry，按照local mask的标准，重新分配指针指向。执行步骤6</em></strong>）。</p><p><img src="eht6.png" alt=""></p><p><strong>STEP 5</strong>：执行步骤6，重新计算<code>IndexOf(key)</code>，由于改变了global depth，新计算的IndexOf(key)是1，最后执行步骤2，判断1指向的bucket没满，执行步骤7，插入23。</p><p><img src="eht7.png" alt=""></p><p><strong>STEP 6</strong>：接下来我们插入11（0b1011）。<strong>NOTE：这个例子在实现的过程中容易忽略掉</strong>。首先计算<code>IndexOf(key)</code>，得到结果为1，我们就要插入红色的bucket。但红色bucket满了，同时，global depth等于local depth，因此需要扩展directory，执行步骤3。</p><p><img src="eht8.png" alt=""></p><p>之前的local depth我们比较最低位的二进制位，将1111和10111放入了一个bucket，由于该bucket产生了overflow，又分裂为两个bucket，我们就需要对这个产生overflow的bucket中的元素重新归类。正常情况下，产生overflow的bucket的中的元素可以被平均的分布到两个bucket中，但这个例子中，我们对比两个数的第二位，发现1111和10111最低的第二位仍然是1，那么还是将两者化为一类。并更新与overflow的bucket相关的directory entry的指向。</p><p><img src="eht9.png" alt=""></p><p>复杂的问题又来了，之前01和11的entry都指向一个bucket，在分裂的时候，我们怎么去redirect呢？答案是利用local depth和local mask。<strong>分裂之前的local depth为1（0b01），意味着指向这个bucket的最低一位二进制位都相同</strong>。01和11两个数的最低一位二进制位都是1。我们要分裂bucket，一定是bucket已经满了，也说明当前比较二进制位最低一位在将来不适用了，因为连上要插入的数，三个数的最低一位二进制位都是1，因此我们才需要local mask，将1（0b01）左移local depth位，变为2（0b10），意味着我们需要考量第二位二进制位才能区分三个数（这里的entry、global depth还有local depth之间的关系比较难理解）。<strong>想要redirect指向同一个bucket的所有entry，我们必须遍历一次directory</strong>。但并不是暴力遍历，通过观察可以发现，01和11刚好相差一个local mask，而且01作为遍历的开始，可以通过<code>hash(key) &amp; (local_mask - 1)</code>计算得到。</p><p>为什么是<code>hash(key) &amp; (local_mask - 1)</code>呢？首先<code>hash(key)</code>可以理解为得到了key的二进制数，local mask是由local depth得到的，local depth表明的是存放在当前bucket中所有key的低位二进制位相等的个数。local mask是下一个需要检查的二进制位的位置。同时我们也知道，既然key能插入这个bucket，<strong>那么说明key和存放于这个bucket中的keys是有共同性的</strong>，这个共同性就是：<strong>低local depth位二进制数完全相同</strong>。local_mask - 1和key的二进制&amp;的结果就是在directory中，最开始的那个entry，因为这个entry的值完全等于<code>hash(key) &amp; (local_mask - 1)</code>。其余所有指向这个bucket的entry，唯一与这个最开始的不同就是：local depth + 1位是0和1的区别。就是相差local mask。</p><p>啊，好复杂，感觉没有说清楚，后续可能更新一下，如果没懂可以私信我或者评论区讨论一下。</p><p>更新<code>IndexOf(key)</code>，由于global index变为2，这时的index就是0b11，即第四个directory。进入步骤2。</p><p><strong>STEP 7</strong>：判断是否能插入蓝色bucket，很明显，bucket又满了，且global depth等于local depth。进行扩展哈希表和分裂bucket。然后分配每一个KV pair。</p><p><img src="eht10.png" alt=""></p><p>最后更新<code>IndexOf(key)</code>，结果是0b011，插入绿色的bucket。</p><p><img src="eht11.png" alt=""></p><p>之后的继续添加和扩展大同小异，重点还是理解entry index、global depth和local depth的深层含义，还有相关位运算的思想。</p><h2 id="踩坑记录"><a href="#踩坑记录" class="headerlink" title="踩坑记录"></a>踩坑记录</h2><ul><li>第一次实现的时候并没有考虑扩展后的指针指向问题，导致程序运行时访问到了nullptr的地址，报错。实际上directory的扩展本质上就是将原来的direcotry完完整整拷贝一份，不同的只是index不同。</li><li>Grade scope做测试的时候，无论如何怎么调试都过不了ConcurrentInsertFindTest和GetNumBucketsTest，尝试着根据在线测试的输出在本地写了若干个对应的测试样例。还是没办法通过。最后阅读别人的文章才发现代码中分配元素的条件和重分配entry指针指向的条件有错误，根本原因还是没理解透彻extendible hash table中的index、global depth和local depth的内涵。以后一定要理解全部的算法内容再考虑代码实现，尤其是细节部分，de这样的bug简直是痛苦。</li><li>在算法的概述中，有涉及到循环插入的过程。在上面的例子中就是插入1011时的情况。判断一个bucket满了，分裂bucket后，将产生overflow的bucket中的元素根据local depth重新分配，结果全部都分配到一个bucket中。这时候如果还是尝试插入1011，是失败的。因此需要通过while迭代，也就是test case中的multi split test。第一次实现的时候并没有考虑到这个问题。</li></ul><p>遵守课程的条例，不公开源码，但我把自己写的相关测试样例放在下面，写的比较粗糙，因为是debug太痛苦时写的。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">TEST</span>(ExtendibleHashTableTest, InsertMultipleSplitTest) &#123;</span><br><span class="line">  <span class="keyword">auto</span> table = std::make_unique&lt;ExtendibleHashTable&lt;<span class="type">int</span>, std::string&gt;&gt;(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">15</span>, <span class="string">&quot;a&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">14</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">23</span>, <span class="string">&quot;c&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">11</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">9</span>, <span class="string">&quot;e&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">4</span>, table-&gt;<span class="built_in">GetNumBuckets</span>());</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">1</span>, table-&gt;<span class="built_in">GetLocalDepth</span>(<span class="number">0</span>));</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">2</span>, table-&gt;<span class="built_in">GetLocalDepth</span>(<span class="number">1</span>));</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">3</span>, table-&gt;<span class="built_in">GetLocalDepth</span>(<span class="number">3</span>));</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">3</span>, table-&gt;<span class="built_in">GetLocalDepth</span>(<span class="number">7</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TEST</span>(ExtendibleHashTableTest, ConcurrentInsertFindTest) &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> num_runs = <span class="number">50</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> num_threads = <span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Run concurrent test multiple times to guarantee correctness.</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> run = <span class="number">0</span>; run &lt; num_runs; run++) &#123;</span><br><span class="line">    <span class="keyword">auto</span> table = std::make_unique&lt;ExtendibleHashTable&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt;(<span class="number">2</span>);</span><br><span class="line">    std::vector&lt;std::thread&gt; threads;</span><br><span class="line">    threads.<span class="built_in">reserve</span>(num_threads);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> tid = <span class="number">0</span>; tid &lt; num_threads; tid++) &#123;</span><br><span class="line">      threads.<span class="built_in">emplace_back</span>([tid, &amp;table]() &#123;</span><br><span class="line">        <span class="type">int</span> val;</span><br><span class="line">        table-&gt;<span class="built_in">Insert</span>(tid, tid);</span><br><span class="line">        <span class="built_in">EXPECT_TRUE</span>(table-&gt;<span class="built_in">Find</span>(tid, val));</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_threads; i++) &#123;</span><br><span class="line">      threads[i].<span class="built_in">join</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">EXPECT_EQ</span>(table-&gt;<span class="built_in">GetGlobalDepth</span>(), <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_threads; i++) &#123;</span><br><span class="line">      <span class="type">int</span> val;</span><br><span class="line">      <span class="built_in">EXPECT_TRUE</span>(table-&gt;<span class="built_in">Find</span>(i, val));</span><br><span class="line">      <span class="built_in">EXPECT_EQ</span>(i, val);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TEST</span>(ExtendibleHashTableTest, ConcurrentInsertFind2Test) &#123;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> num_runs = <span class="number">30</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> num_threads = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Run concurrent test multiple times to guarantee correctness.</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> run = <span class="number">0</span>; run &lt; num_runs; run++) &#123;</span><br><span class="line">    <span class="keyword">auto</span> table = std::make_unique&lt;ExtendibleHashTable&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt;(<span class="number">2</span>);</span><br><span class="line">    std::vector&lt;std::thread&gt; threadsInsert;</span><br><span class="line">    std::vector&lt;std::thread&gt; threadsFind;</span><br><span class="line">    threadsInsert.<span class="built_in">reserve</span>(num_threads);</span><br><span class="line">    threadsFind.<span class="built_in">reserve</span>(num_threads);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> tid = <span class="number">0</span>; tid &lt; num_threads; tid++) &#123;</span><br><span class="line">      threadsInsert.<span class="built_in">emplace_back</span>([tid, &amp;table]() &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = tid * <span class="number">10</span>; i &lt; (tid + <span class="number">1</span>) * <span class="number">10</span>; i++) &#123;</span><br><span class="line">          table-&gt;<span class="built_in">Insert</span>(i, i);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_threads; i++) &#123;</span><br><span class="line">      threadsInsert[i].<span class="built_in">join</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> tid = <span class="number">0</span>; tid &lt; num_threads; tid++) &#123;</span><br><span class="line">      threadsFind.<span class="built_in">emplace_back</span>([tid, &amp;table]() &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = tid * <span class="number">10</span>; i &lt; (tid + <span class="number">1</span>) * <span class="number">10</span>; i++) &#123;</span><br><span class="line">          <span class="type">int</span> val;</span><br><span class="line">          <span class="built_in">EXPECT_TRUE</span>(table-&gt;<span class="built_in">Find</span>(i, val));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_threads; i++) &#123;</span><br><span class="line">      threadsFind[i].<span class="built_in">join</span>();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">TEST</span>(ExtendibleHashTableTest, GetNumBucketsTest) &#123;</span><br><span class="line">  <span class="keyword">auto</span> table = std::make_unique&lt;ExtendibleHashTable&lt;<span class="type">int</span>, std::string&gt;&gt;(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">4</span>, <span class="string">&quot;a&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">12</span>, <span class="string">&quot;b&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">16</span>, <span class="string">&quot;c&quot;</span>);</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">4</span>, table-&gt;<span class="built_in">GetNumBuckets</span>());</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">64</span>, <span class="string">&quot;d&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">31</span>, <span class="string">&quot;e&quot;</span>);</span><br><span class="line"></span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">10</span>, <span class="string">&quot;f&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">51</span>, <span class="string">&quot;g&quot;</span>);</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">4</span>, table-&gt;<span class="built_in">GetNumBuckets</span>());</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">15</span>, <span class="string">&quot;h&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">18</span>, <span class="string">&quot;i&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">20</span>, <span class="string">&quot;j&quot;</span>);</span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">7</span>, table-&gt;<span class="built_in">GetNumBuckets</span>());</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">7</span>, <span class="string">&quot;k&quot;</span>);</span><br><span class="line">  table-&gt;<span class="built_in">Insert</span>(<span class="number">23</span>, <span class="string">&quot;l&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">EXPECT_EQ</span>(<span class="number">8</span>, table-&gt;<span class="built_in">GetNumBuckets</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Database Management System </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Monodepth2 Paper Note</title>
      <link href="/2022/12/23/Monodepth2/"/>
      <url>/2022/12/23/Monodepth2/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Abstract</strong>: Per-pixel ground truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods.</p><p>Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reproduction loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.</p></blockquote><h2 id="1-先验知识"><a href="#1-先验知识" class="headerlink" title="1. 先验知识"></a>1. 先验知识</h2><p>对于自监督或者说无监督学习的单目图像深度估计任务而言，存在两种现有的训练方案。</p><blockquote><ol><li><strong>stereo pairs</strong> 即立体图像对，包含一张左边照相机的图片和一张右边照相机的图片。</li><li><strong>monocular video</strong> 即单目的视频流数据。</li></ol></blockquote><p>对于<strong>monocular video</strong>来说，为了估计图像的深度，由于缺乏相机运动的先验知识，所以模型同样需要估计相机的姿态变化。这通常会额外训练一个叫作 <strong>pose estimation network</strong>_ 的网络，即姿态估计网络。网络的输入是一系列的图像帧，网络的输出是对应的相机变换方式。</p><p><strong>什么是 ill-posed 问题？</strong>不适定问题（ill-posed problem）和适定问题（well-posed problem）是数学领域对问题进行定义的术语。不满足以下三点的任意一点，都是ill-posed问题：</p><blockquote><ol><li>A solution exists. 有解</li><li>The solution is unique. 解唯一</li><li>The solution’s behavior changes continuously with the initial conditions. 解稳定</li></ol></blockquote><p><strong>为什么深度估计是ill-posed问题？</strong> 因为深度估计对于每一张图片会有多个解，且不稳定。</p><p><strong>Occluded pixels</strong>：遮盖的像素点。在某些序列图下，会出现在一张图没有被遮挡，而在另一张图被遮挡的像素点。</p><p><strong>Out of view pixels</strong>：出界的像素点。由于相机的运动，导致某些像素点不在另一张图像上。</p><h2 id="2-本文的主要贡献"><a href="#2-本文的主要贡献" class="headerlink" title="2. 本文的主要贡献"></a>2. 本文的主要贡献</h2><blockquote><ol><li>当使用单目监督时，会产生像素遮盖的现象。为解决这一问题，提出了<strong>外观匹配损失函数</strong>。</li><li>提出了<strong>自动掩码</strong>的方法，可以忽略那些和相机运动无关的像素点。</li><li>提出了<strong>多尺度的外观匹配损失函数</strong>，可以降低深度的暇疵。</li></ol></blockquote><h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><h3 id="3-1-自监督学习"><a href="#3-1-自监督学习" class="headerlink" title="3.1 自监督学习"></a>3.1 自监督学习</h3><p>自监督深度估计将学习问题作为一种视图合成问题，即通过一个其他视角的图片来预测目标图片的样子。使用一个中间变量——视差或深度，来限制网络在图像合成任务的学习过程，最后我们就可以提取出这个中间变量，转化成图像的深度图片。这是一个ill-posed的问题，因为如果确定了相机的相对姿态，会有很多图片（图片中的每个像素的深度都不一致）都可以合成出对应视角下的目标图片。经典的双目甚至多目方法通过强化深度图片的平滑度以及计算图片的一致性解决了这个问题。</p><p>该工作仍继续沿用之前的思想，将任务作为视图合成问题，通过目标图像和重建得到的目标图像之间的误差作为学习的指导。</p><p>首先，可以将源图像$I_{t’}$与目标图像$I_{t}$之间相机的相对姿态表示为$T_{t \rightarrow {t^{‘} } }$。通过预测目标图像的深度图$D_t$，最小化目标图像以及不同的源图像重建出的目标图像之间的误差，实现深度的预测。数学描述为</p><script type="math/tex; mode=display">L_p = \sum_{t'}reconError(I_t, I_{ {t^{'} } \rightarrow t}) \tag 1</script><script type="math/tex; mode=display">I_{ {t^{'} } \rightarrow t} = I_{t^{'} } ( proj(D_t, T_{t \rightarrow {t^{'} } }, K) )\tag 2</script><p>这里的$proj(D_t, T_{t \rightarrow {t^{‘}}}, K)$可以进一步拆解为如下公式</p><script type="math/tex; mode=display">D_t = DispToDepth(disparity)</script><script type="math/tex; mode=display">CamPoints = BackprojectDepth(D_t, K^{-1})</script><script type="math/tex; mode=display">2Dcoord = Project3D(CamPoints, K, T_{t \rightarrow {t^{'} } })</script><script type="math/tex; mode=display">I_{ {t^{'} } \rightarrow t} = I_{t^{'} } ( 2Dcoord ) \tag 3</script><p>那么如何理解公示(3)呢？</p><p>第一行的disparity就是Network预测的结果。首先，我们需要对disparity做一个转化。由先验知识可知</p><script type="math/tex; mode=display">depth = \frac{bf}{disparity} \tag 4</script><p>可见depth和disparity呈现反比关系。这里使用<code>DispToDepth</code>函数实现转换。</p><p>第二行做了一个前提假设，假设我们拍摄target图像的相机在世界坐标系的原点处。根据《视觉SLAM十四讲》第五讲中所述，三维世界的坐标系可以通过相机的内参矩阵转化为二维坐标。数学描述为</p><script type="math/tex; mode=display">ZP_{uv}= Z\left[\begin{matrix}u\\v\\1\end{matrix}\right]=\left[\begin{matrix}f_x & 0   & c_x \\0   & f_y & c_y \\0   & 0   & 1   \\\end{matrix}\right]\left[\begin{matrix}X \\Y \\Z\\\end{matrix}\right]=KP_w \tag 5</script><p>其中P为世界坐标，u、v为相机坐标。而我们现在有了像素的二维坐标，可以通过<code>np.meshgrid</code>构建。为了得到对应的世界坐标，我们不仅仅需要像素的二维坐标，还需要一个深度。其实，公式(5)可以写成</p><script type="math/tex; mode=display">P_{uv}=\left[\begin{matrix}u\\v\\1\end{matrix}\right]=\left[\begin{matrix}f_x & 0   & c_x \\0   & f_y & c_y \\0   & 0   & 1   \\\end{matrix}\right]\left[\begin{matrix}X/Z \\Y/Z \\1\\\end{matrix}\right]=KP^{'}_w \tag 5</script><p>$P^{‘}_w$称为归一化坐标。归一化坐标可以看成相机前方$z=1$处平面上的坐标。可以看到，如果对归一化坐标同时乘以任何一个数，相机的归一化坐标是完全一样的，说明<strong>深度信息</strong>在单目图像上丢失了。</p><p>深度就是需要通过公式(3)第一行得到的depth了。首先用<code>np.meshgrid</code>得到像素坐标，和内参的逆矩阵相乘得到归一化坐标，归一化坐标乘以深度，就可以得到三维世界坐标系下的坐标（注意这里<strong>假设我们拍摄target图像的相机就是世界坐标系</strong>），即CamPoints。数学描述为</p><script type="math/tex; mode=display">P_w = ZK^{-1}P_{uv} \tag 6</script><p>我们现在有了世界坐标了，接下来让我们移动相机，假设我们简单的把相机平移到原相机位置的右侧，这时候就可以用先验知识求得相机位姿的矩阵$T_{t \rightarrow {t^{‘} } }$。当然，更细节的得到位姿矩阵的方法在<code>transformation_from_parameters</code>函数中实现，需要结合<strong>轴角</strong>和<strong>平移向量</strong>构建。接下来是理解模型是如何监督的重点。我们通过左侧相机的像素坐标得到了每一个像素位置的世界坐标。现在我们需要知道，左侧图像中的每一个像素点所对应在空间中的实际的点，映射到了右侧相机的图片的哪个像素位置。可以继续通过公式(5)变换到像素坐标系下，但这里的右侧相机所在的坐标系已经与标准的世界坐标系有所偏移，所以我们需要做一些小小的修改</p><script type="math/tex; mode=display">P_{u^{'}v^{'} }=\frac{1}{Z}K(RP_w+\vec t)=\frac{1}{Z}KT_{t \rightarrow {t^{'} } }P_w \tag 7</script><p>具体在<code>Project3D</code>中实现。这里得到的新的$P_{u^{‘} v^{‘}}$，就是左侧图像像素$P_{uv}$所表示的空间点，在右侧图像像素的位置，即右侧图像的$(u’,v’)$处。之后通过sampler采样，得到根据右侧图像重建后的图像$I_{ {t^{ ‘ } } \rightarrow t}$。</p><p>结合图1更好理解</p><p><img src="project3d.jpeg" alt=""></p><center>  Fig. 1 监督过程</center><p>对于重建误差，可以表述为</p><script type="math/tex; mode=display">reconError(I_a,I_b)=\frac{\alpha}{2}(1-SSIM(I_a, I_b)) + (1-\alpha)||I_a - I_b|| \tag 8</script><p>边缘平滑损失定义为</p><script type="math/tex; mode=display">L_s = |\partial_xd^*_t|e^{-|\partial_xI_t|}+|\partial_yd^*_t|e^{-|\partial_yI_t|} \tag 9</script><p>最后总的目标函数为</p><script type="math/tex; mode=display">L = \mu L_p +\lambda L_s \tag{10}</script><h3 id="3-2-自监督学习优化"><a href="#3-2-自监督学习优化" class="headerlink" title="3.2 自监督学习优化"></a>3.2 自监督学习优化</h3><p><img src="monodepth2model.png" alt=""></p><center>  Fig. 2 (a)深度预测网络。(b)相机位姿网络。(c)最小重建投影误差。(d)多尺度估计。</center><h4 id="3-2-1-逐像素最小重建投影误差"><a href="#3-2-1-逐像素最小重建投影误差" class="headerlink" title="3.2.1 逐像素最小重建投影误差"></a>3.2.1 逐像素最小重建投影误差</h4><p><strong>Problematic pixels</strong>可以分为两类，一种是超出图像边界的像素(<strong>Out of view pixels</strong>)，另一种是遮挡像素(<strong>Occluded pixels</strong>)。超出图像边界的像素可以通过掩盖这些像素的误差，即不计入误差累计。但并没有解决遮挡像素的问题。</p><p>计算重建投影误差的时候，之前的一些方法都是把不同源图像与目标图像的误差平均。这种情况下，如果网络预测出目标图像的某一个像素点A正确的深度，经过源图像的采样后，重建出的像素点可能会像图2的(c)的$I_{t-1}$所示，导致采样到了遮挡像素的部位，从而造成对应位置像素值差别很大，对结果造成一定的影响。所以相较于公式(1)，该工作做了如下改进</p><script type="math/tex; mode=display">L_p = min(reconError(I_t, I_{ {t^{'} } \rightarrow t}) \tag{11})</script><p><img src="Monodepth2Loss.png" alt="Screenshot 2022-11-16 at 13.00.20"></p><center>  Fig. 3 最小重建投影误差。每个像素都会根据其最匹配的源图像进行计算。图中L画圈部位在R中属于遮挡像素，但可以在-1中找到相匹配的像素点。本质而言是充分的利用了不同源图像的信息。</center><p>这种改进可以将超出边界的像素和遮挡像素问题一举解决，且可以见效图片边界的瑕疵、提升遮挡边界的清晰度并且可以提高精度。</p><h4 id="3-2-2-静态像素自动掩码"><a href="#3-2-2-静态像素自动掩码" class="headerlink" title="3.2.2 静态像素自动掩码"></a>3.2.2 静态像素自动掩码</h4><p>自监督学习的一个前提假设是，场景是静止的，相机是运动的。当相机是静止的或场景中有运动的物体时，性能就会受到很大的影响（测试时会产生黑洞）。一个很简单的想法就是，把这一帧到下一帧中不变的像素点掩盖。同于先前的工作，也是将每个像素点加入掩码算子$\mu$。但不同的是，先前工作需要通过学习得到$\mu$，而该工作是通过前向传播过程自动计算得到，且只有0和1两个值。观察得到，如果在相邻两帧中像素点保持相同会有三种情况：第一种是相机静止；第二种是物体和相机保持同样的速度和方向，相对静止；第三种是低纹理区域。</p><h4 id="3-2-3-多尺度估计"><a href="#3-2-3-多尺度估计" class="headerlink" title="3.2.3 多尺度估计"></a>3.2.3 多尺度估计</h4><p>之前工作的多尺度估计都是在不同size之下计算好误差，最后平均。而这样会倾向于在大面积的low-texture区域产生黑洞，也会造成瑕疵。因此该工作将不同size的预测的图片resize到原始图片的大小，在相同的尺度下进行计算。</p><h3 id="3-3-其他考虑"><a href="#3-3-其他考虑" class="headerlink" title="3.3 其他考虑"></a>3.3 其他考虑</h3><p>网络的baseline采用U-net的encoder，decoder架构，加入了跳层连接，以便更好的结合深度特征信息和局部特征。使用ResNet18作为encoder，包含11M参数量，并且采用了ImageNet上预训练好的权重，实验表明预训练的结果要比从一开始训练的结果要好。网络的decoder采用和Unsupervised monocular depth estimation with left- right consistency中的类似，但最后一层加入sigmoid输出，其他采用ELU作为激活函数。Decoder中用反射padding代替zero-padding，实验表明效果不错。</p><p>对于姿态网络，网络输出轴角和平移向量，并缩放0.01。</p><p>数据增强的概率为50%，策略为水平翻转、随机亮度、对比度、饱和度以及hue jitter。所有输入网络的图片都会用相同的参数进行增强。</p><p>网络用pytorch实现，优化器为Adam，epoch为20，batchsize为12，输入输出默认为640x192。前15epoch用0.0001学习率，最后五个为0.00001。平滑$\lambda$为0.001。 </p><h3 id="3-4-数据集"><a href="#3-4-数据集" class="headerlink" title="3.4 数据集"></a>3.4 数据集</h3><h2 id="4-源码解析"><a href="#4-源码解析" class="headerlink" title="4. 源码解析"></a>4. 源码解析</h2><h3 id="4-1-layers-py"><a href="#4-1-layers-py" class="headerlink" title="4.1 layers.py"></a>4.1 layers.py</h3><p><code>layers.py</code>文件是Monodepth2中最为核心的一个文件，其中包含了以下几种函数：</p><blockquote><p><code>disp_to_dpeth(disp, min_depth, max_depth)</code>：它会将网络的simoid输出转化为预测的深度，这里是运用了深度和视察的先验关系。</p><p><code>transformation_from_parameters(axisangle, translation, invert)</code>：根据poseNet预测出的角度和平移量，计算4x4的转换矩阵。</p><p><code>rot_from_axisangle(vec)</code>：根据坐标轴的欧拉角，得到4x4的旋转矩阵。</p><p><code>get_translation_matrix(translation_vector)</code>：把预测出的平移量转化为4x4的平移矩阵。</p><p><code>upsample(x)</code>：将输入的张量用最邻近差值实现上采样。</p><p><code>get_smooth_loss(disp, img)</code>：计算视差图的平滑度。</p><p><code>compute_depth_errors(gt, pred)</code>：计算预测出的深度图片和GT的各项衡量指标的值。</p></blockquote><p>包含以下层：</p><blockquote><p>Conv3x3：3x3卷积计算单元。</p><p>BackprojectDepth：根据预测的深度、相机坐标系下的坐标和相机内参矩阵的逆矩阵，计算空间坐标系的矩阵（4维度，最后一维度为1，表示三维空间的点）。</p><p>Project3D：根据转换矩阵T和相机内参矩阵K，以及三维空间坐标，计算得到对应相机坐标系下的坐标。</p><p>SSIM：结构相似性计算层。</p></blockquote><h4 id="disp-to-depth"><a href="#disp-to-depth" class="headerlink" title="disp_to_depth"></a>disp_to_depth</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">disp_to_depth</span>(<span class="params">disp, min_depth, max_depth</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert network&#x27;s sigmoid output into depth prediction</span></span><br><span class="line"><span class="string">    The formula for this conversion is given in the &#x27;additional considerations&#x27;</span></span><br><span class="line"><span class="string">    section of the paper.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 将预测得到的视差通过min_depth和max_depth的限制，得到对应范围内的深度图</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> we know that disp = f*b / depth, but in this function, where are f and b?</span></span><br><span class="line">    min_disp = <span class="number">1</span> / max_depth</span><br><span class="line">    max_disp = <span class="number">1</span> / min_depth</span><br><span class="line">    scaled_disp = min_disp + (max_disp - min_disp) * disp</span><br><span class="line">    depth = <span class="number">1</span> / scaled_disp</span><br><span class="line">    <span class="keyword">return</span> scaled_disp, depth</span><br></pre></td></tr></table></figure><h4 id="transformation-from-parameters"><a href="#transformation-from-parameters" class="headerlink" title="transformation_from_parameters"></a>transformation_from_parameters</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformation_from_parameters</span>(<span class="params">axisangle, translation, invert=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert the network&#x27;s (axisangle, translation) output into a 4x4 matrix</span></span><br><span class="line"><span class="string">     一般而言，对于一个坐标，可以通过旋转矩阵R和平移向量t来变换到另一个坐标</span></span><br><span class="line"><span class="string">     但是也可以将R和t写作齐次式，M</span></span><br><span class="line"><span class="string">     函数的输入是欧拉角，需要调用 rot_from_axisangle将欧拉角转化为旋转矩阵</span></span><br><span class="line"><span class="string">     另一个输入是平移向量，需要调用get_translation_matrix将向量转化为平移矩阵</span></span><br><span class="line"><span class="string">     最后将两个矩阵结合即可</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    R = rot_from_axisangle(axisangle)</span><br><span class="line">    t = translation.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        R = R.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        t *= -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    T = get_translation_matrix(t)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invert:</span><br><span class="line">        M = torch.matmul(R, T)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        M = torch.matmul(T, R)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> M</span><br></pre></td></tr></table></figure><h4 id="rot-from-axisangle"><a href="#rot-from-axisangle" class="headerlink" title="rot_from_axisangle"></a>rot_from_axisangle</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rot_from_axisangle</span>(<span class="params">vec</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert an axisangle rotation into a 4x4 transformation matrix</span></span><br><span class="line"><span class="string">    (adapted from https://github.com/Wallacoloo/printipi)</span></span><br><span class="line"><span class="string">    Input &#x27;vec&#x27; has to be Bx1x3</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    angle = torch.norm(vec, <span class="number">2</span>, <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">    axis = vec / (angle + <span class="number">1e-7</span>)</span><br><span class="line"></span><br><span class="line">    ca = torch.cos(angle)</span><br><span class="line">    sa = torch.sin(angle)</span><br><span class="line">    C = <span class="number">1</span> - ca</span><br><span class="line"></span><br><span class="line">    x = axis[..., <span class="number">0</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    y = axis[..., <span class="number">1</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    z = axis[..., <span class="number">2</span>].unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    xs = x * sa</span><br><span class="line">    ys = y * sa</span><br><span class="line">    zs = z * sa</span><br><span class="line">    xC = x * C</span><br><span class="line">    yC = y * C</span><br><span class="line">    zC = z * C</span><br><span class="line">    xyC = x * yC</span><br><span class="line">    yzC = y * zC</span><br><span class="line">    zxC = z * xC</span><br><span class="line"></span><br><span class="line">    rot = torch.zeros((vec.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>)).to(device=vec.device)</span><br><span class="line"></span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">0</span>] = torch.squeeze(x * xC + ca)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">1</span>] = torch.squeeze(xyC - zs)</span><br><span class="line">    rot[:, <span class="number">0</span>, <span class="number">2</span>] = torch.squeeze(zxC + ys)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">0</span>] = torch.squeeze(xyC + zs)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">1</span>] = torch.squeeze(y * yC + ca)</span><br><span class="line">    rot[:, <span class="number">1</span>, <span class="number">2</span>] = torch.squeeze(yzC - xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">0</span>] = torch.squeeze(zxC - ys)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">1</span>] = torch.squeeze(yzC + xs)</span><br><span class="line">    rot[:, <span class="number">2</span>, <span class="number">2</span>] = torch.squeeze(z * zC + ca)</span><br><span class="line">    rot[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> rot</span><br></pre></td></tr></table></figure><h4 id="get-translation-matrix"><a href="#get-translation-matrix" class="headerlink" title="get_translation_matrix"></a>get_translation_matrix</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_translation_matrix</span>(<span class="params">translation_vector</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert a translation vector into a 4x4 transformation matrix</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    T = torch.zeros(translation_vector.shape[<span class="number">0</span>], <span class="number">4</span>, <span class="number">4</span>).to(device=translation_vector.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转为列向量</span></span><br><span class="line">    t = translation_vector.contiguous().view(-<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    T[:, <span class="number">0</span>, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">1</span>, <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">2</span>, <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line">    T[:, <span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 给T矩阵最后一列的前三个赋值为列向量t</span></span><br><span class="line">    T[:, :<span class="number">3</span>, <span class="number">3</span>, <span class="literal">None</span>] = t</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> T</span><br></pre></td></tr></table></figure><h4 id="get-smooth-loss"><a href="#get-smooth-loss" class="headerlink" title="get_smooth_loss"></a>get_smooth_loss</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_smooth_loss</span>(<span class="params">disp, img</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the smoothness loss for a disparity image</span></span><br><span class="line"><span class="string">    The color image is used for edge-aware smoothness</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算x方向的视差的梯度</span></span><br><span class="line">    grad_disp_x = torch.<span class="built_in">abs</span>(disp[:, :, :, :-<span class="number">1</span>] - disp[:, :, :, <span class="number">1</span>:])</span><br><span class="line">    <span class="comment"># 计算y方向的视差的梯度</span></span><br><span class="line">    grad_disp_y = torch.<span class="built_in">abs</span>(disp[:, :, :-<span class="number">1</span>, :] - disp[:, :, <span class="number">1</span>:, :])</span><br><span class="line"></span><br><span class="line">    grad_img_x = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :, :-<span class="number">1</span>] - img[:, :, :, <span class="number">1</span>:]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    grad_img_y = torch.mean(torch.<span class="built_in">abs</span>(img[:, :, :-<span class="number">1</span>, :] - img[:, :, <span class="number">1</span>:, :]), <span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    grad_disp_x *= torch.exp(-grad_img_x)</span><br><span class="line">    grad_disp_y *= torch.exp(-grad_img_y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_disp_x.mean() + grad_disp_y.mean()</span><br></pre></td></tr></table></figure><h4 id="BackprojectDepth"><a href="#BackprojectDepth" class="headerlink" title="BackprojectDepth"></a>BackprojectDepth</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BackprojectDepth</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将预测得到的depth图转化为3维的点云图片</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width</span>):</span><br><span class="line">        <span class="built_in">super</span>(BackprojectDepth, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.height = height</span><br><span class="line">        self.width = width</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据宽度和高度，生成对应的行列坐标，会得到[列坐标2维矩阵，行坐标2维矩阵]这样一个list</span></span><br><span class="line">        meshgrid = np.meshgrid(<span class="built_in">range</span>(self.width), <span class="built_in">range</span>(self.height), indexing=<span class="string">&#x27;xy&#x27;</span>)</span><br><span class="line">        <span class="comment"># 把list按照第一维度堆叠起来，生成shape为[2, width, height]的id_coords</span></span><br><span class="line">        self.id_coords = np.stack(meshgrid, axis=<span class="number">0</span>).astype(np.float32)</span><br><span class="line">        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),</span><br><span class="line">                                      requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        self.ones = nn.Parameter(torch.ones(self.batch_size, <span class="number">1</span>, self.height * self.width),</span><br><span class="line">                                 requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将id_coords的列坐标和行坐标先打平为[1, width*height]，堆叠为[2, width*height]，扩充为[1,           # 2, width*height]</span></span><br><span class="line">        self.pix_coords = torch.unsqueeze(torch.stack(</span><br><span class="line">            [self.id_coords[<span class="number">0</span>].view(-<span class="number">1</span>), self.id_coords[<span class="number">1</span>].view(-<span class="number">1</span>)], <span class="number">0</span>), <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 按照batch_size堆叠</span></span><br><span class="line">        self.pix_coords = self.pix_coords.repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将1张量与坐标结合，这样形成[1, 3, width*height]的张量，每一列就代表一个[x, y, 1]二维坐标</span></span><br><span class="line">        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], <span class="number">1</span>),</span><br><span class="line">                                       requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, depth, inv_K</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            u   fx  0   cx     X/Z</span></span><br><span class="line"><span class="string">            v = 0   fy  cy  .  Y/Z</span></span><br><span class="line"><span class="string">            1   0   0   1      1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            pix_coords = K . cam_points</span></span><br><span class="line"><span class="string">            cam_points = K-1 . pix_coords</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        cam_points = torch.matmul(inv_K[:, :<span class="number">3</span>, :<span class="number">3</span>], self.pix_coords)</span><br><span class="line">        <span class="comment"># 上一行得到的cam_points，本质上是在归一化平面上的，此时Z即深度信息是丢失的，这也是</span></span><br><span class="line">        <span class="comment"># 单目图像无法得到3维图像的原因。但这里的depth是经过神经网络预测得到的，因此对于归一化</span></span><br><span class="line">        <span class="comment"># 平面上的坐标[X/Z, Y/Z, 1]同时乘各个点的深度，就能得到[X, Y, Z]</span></span><br><span class="line">        cam_points = depth.view(self.batch_size, <span class="number">1</span>, -<span class="number">1</span>) * cam_points</span><br><span class="line">        <span class="comment"># [X, Y, Z] -&gt; [X, Y, Z, 1]</span></span><br><span class="line">        cam_points = torch.cat([cam_points, self.ones], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cam_points</span><br></pre></td></tr></table></figure><h4 id="Project3D"><a href="#Project3D" class="headerlink" title="Project3D"></a>Project3D</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Project3D</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Layer which projects 3D points into a camera with intrinsics K and at position T</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, height, width, eps=<span class="number">1e-7</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Project3D, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.height = height</span><br><span class="line">        self.width = width</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, points, K, T</span>):</span><br><span class="line">        <span class="comment"># 传入的points为世界坐标系下的坐标[X, Y, Z, 1]</span></span><br><span class="line">        <span class="comment"># K为相机内参，T为转换矩阵</span></span><br><span class="line">        M = torch.matmul(K, T)[:, :<span class="number">3</span>, :]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 相机坐标系下的坐标 = K (RP+t) = KTP</span></span><br><span class="line">        cam_points = torch.matmul(M, points)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 除去Z，eps是为了防止除0导致的错误</span></span><br><span class="line">        pix_coords = cam_points[:, :<span class="number">2</span>, :] / (cam_points[:, <span class="number">2</span>, :].unsqueeze(<span class="number">1</span>) + self.eps)</span><br><span class="line">        <span class="comment"># 从[batch, 2, width*height]转换为[batch, 2, height, width]</span></span><br><span class="line">        pix_coords = pix_coords.view(self.batch_size, <span class="number">2</span>, self.height, self.width)</span><br><span class="line">        <span class="comment"># [batch, 2, height, width] -&gt; [batch, height, width, 2]</span></span><br><span class="line">        pix_coords = pix_coords.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 归一化到0-1之间</span></span><br><span class="line">        pix_coords[..., <span class="number">0</span>] /= self.width - <span class="number">1</span></span><br><span class="line">        pix_coords[..., <span class="number">1</span>] /= self.height - <span class="number">1</span></span><br><span class="line">        <span class="comment"># 移动到[-1, 1]之间</span></span><br><span class="line">        pix_coords = (pix_coords - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> pix_coords</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Paper Reading Note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Structural SIMilarity Index</title>
      <link href="/2022/11/10/SSIM/"/>
      <url>/2022/11/10/SSIM/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Over-all-Purpose"><a href="#1-Over-all-Purpose" class="headerlink" title="1. Over all Purpose"></a>1. Over all Purpose</h2><p>The luminance information computation method is:</p><script type="math/tex; mode=display">Lumination = illumination\times reflectance \tag 1</script><p>However, based on our prior knowledge</p><blockquote><p><strong>The structures of the objects in the scene are independent of the illumination.</strong></p></blockquote><p>Then we can define the structural information in an image as those attributes that represent the structure of objects in the scene. (Since luminance and contrast can vary across a scene, we can use the local luminance and contrast.)</p><h2 id="2-System-Diagram"><a href="#2-System-Diagram" class="headerlink" title="2. System Diagram"></a>2. System Diagram</h2><p><img src="SSIMDiagram.png" alt="SSIM"></p><center>  Fig. 1 Diagram of the structural similarity (SSIM) measurement system.</center><p>The system separates the task of similarity measurement into three comparisons: <strong>luminance</strong>, <strong>contrast</strong> and <strong>structure</strong>.</p><h2 id="3-Details"><a href="#3-Details" class="headerlink" title="3. Details"></a>3. Details</h2><h3 id="Three-Important-conditions"><a href="#Three-Important-conditions" class="headerlink" title="Three Important conditions"></a>Three Important conditions</h3><ol><li>Symmetry: $S(x,y)=S(y,x)$;</li><li>Boundedness: $S(x, y)\leq1$;</li><li>Unique maximum: $S(x, y)=1$ if and only if $x=y$;</li></ol><h3 id="Luminance-Measurement"><a href="#Luminance-Measurement" class="headerlink" title="Luminance Measurement"></a>Luminance Measurement</h3><script type="math/tex; mode=display">\mu_x = \frac{1}{N}\sum_{i=1}^Nx_i \tag 2</script><h3 id="Luminace-Comparison"><a href="#Luminace-Comparison" class="headerlink" title="Luminace Comparison"></a>Luminace Comparison</h3><script type="math/tex; mode=display">l(x, y)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1} \tag 3</script><p>where the constance $C_1$ is included to avoid instability when $\mu_x^2+\mu_y^2$ is very close to zero.</p><script type="math/tex; mode=display">C_1 = (K_1L)^2 \tag 4</script><p>where $L$ is the dynamic range of the pixel values, and $K_1&lt;&lt;1$ is a small constant.</p><h3 id="Contrast-Measurement"><a href="#Contrast-Measurement" class="headerlink" title="Contrast Measurement"></a>Contrast Measurement</h3><p>Remove the intensity from the signal. So in discrete form, the resulting signal $x-\mu_x$ corresponds to the projection of vector $x$ onto the hyperplane defined by</p><script type="math/tex; mode=display">\sum_{i=1}^Nx_i = 0 \tag 5</script><p>Use the standard deviation as an estimate of the contrast (unbiased estimate)</p><script type="math/tex; mode=display">\sigma_x = (\frac{1}{N-1}\sum_{i=1}^N(x_i - \mu_x)^2)^{1/2} \tag 6</script><h3 id="Contrast-Comparison"><a href="#Contrast-Comparison" class="headerlink" title="Contrast Comparison"></a>Contrast Comparison</h3><script type="math/tex; mode=display">c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2} \tag 7</script><script type="math/tex; mode=display">C_2 = (K_2L)^2 \tag 8</script><h3 id="Structure-Comparison"><a href="#Structure-Comparison" class="headerlink" title="Structure Comparison"></a>Structure Comparison</h3><script type="math/tex; mode=display">s(x,y) = \frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3} \tag 9</script><script type="math/tex; mode=display">\sigma_{xy} = \frac{1}{N - 1}\sum_{i=1}^N(x_i - \mu_x)(y_i-\mu_y) \tag {10}</script><h3 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h3><script type="math/tex; mode=display">SSIM(x,y) = [l(x,y)]^{\alpha}\cdot[c(x,y)]^{\beta}\cdot[s(x,y)]^{\gamma}</script><p>Where $\alpha &gt; 0, \beta &gt; 0$, and $\gamma &gt; 0$ are parameters used to adjust the relative importance of the three components.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper Reading Note </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
